# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tYwwh4fXDEBbMSnM7nUKcWklaoJWG38s
"""

#Generate Train Data
import csv
train_text = []
train_label = []
with open('train.csv', 'r') as file:
    reader = csv.reader(file,delimiter='\t')
    ind=0
    for row in reader:
      if ind==0:
        ind = ind+1
        continue
      train_text.append(row[2])
      train_label.append(int(row[3]))

#Generate Valid Data
valid_text = []
valid_label = []
with open('val.csv', 'r') as file:
    reader = csv.reader(file,delimiter='\t')
    ind=0
    for row in reader:
      if ind==0:
        ind = ind+1
        continue
      valid_text.append(row[2])
      valid_label.append(int(row[3]))

#Generate Test data
test_text = []
test_label = []
with open('test.csv', 'r') as file:
    reader = csv.reader(file,delimiter='\t')
    ind=0
    for row in reader:
      if ind==0:
        ind = ind+1
        continue
      test_text.append(row[2])
      test_label.append(int(row[3]))

content=open("CBOW_WORDS.txt").readlines()#File generated from cbow.py 

words = []
for word in content:
  words.append(word.split("\n")[0])
words = sorted(words,reverse=False) # To be sorted for LSTM and Skip gram embeddings only else comment this line
#Generate Lookup Dictionary
word_to_idx = {}
for i in range(len(words)):
  word_to_idx[words[i]] = i

import torch
embedding = torch.load("embedding.pt") #To be uploaded for all 3 methods of embeddings
'''
#To be uncommented for gloVe Embeddings method
from torchtext.vocab import GloVe
embedding_glove = GloVe(name='6B', dim=100) #To upload GloVe Embeddings
'''

import torch
train_embed = []
for sentences in train_text:
  sentence_embed=[]
  for word in sentences:
    if(word in word_to_idx.keys()):#To be commented fro GloVe Embeddings method
      sentence_embed.append(embedding[word_to_idx[word]]) #To be commented fro GloVe Embeddings method
    '''
    #To be uncommented for GloVe Embeddings
    try:
      sentence_embed.append(embedding_glove[word])
    except KeyError:
      print(word)
    '''
  if(len(sentence_embed)!=0):
    sentence_embed = torch.stack(sentence_embed)
  else:
    sentence_embed = torch.empty(0)

  train_embed.append(sentence_embed)

valid_embed = []
for sentences in valid_text:
  sentence_embed=[]
  for word in sentences:
    if(word in word_to_idx.keys()):#To be commented fro GloVe Embeddings method
      sentence_embed.append(embedding[word_to_idx[word]])#To be commented fro GloVe Embeddings method
    '''
    #To be uncommented for GloVe Embeddings
    try:
      sentence_embed.append(embedding_glove[word])
    except KeyError:
      print(word)
    '''
  if(len(sentence_embed)!=0):
    sentence_embed = torch.stack(sentence_embed)
  else:
    sentence_embed = torch.empty(0)
  valid_embed.append(sentence_embed)

test_embed = []
for sentences in test_text:
  sentence_embed=[]
  for word in sentences:
    if(word in word_to_idx.keys()):#To be commented fro GloVe Embeddings method
      sentence_embed.append(embedding[word_to_idx[word]])#To be commented fro GloVe Embeddings method
    '''
    #To be uncommented for GloVe Embeddings
    try:
      sentence_embed.append(embedding_glove[word])
    except KeyError:
      print(word)
    '''
  if(len(sentence_embed)!=0):
    sentence_embed = torch.stack(sentence_embed)
  else:
    sentence_embed = torch.empty(0)
  test_embed.append(sentence_embed)

#Convert list to Tensors
import torch
import torch.nn as nn
import torch.nn.functional as F
import os


SEED = 1234
torch.manual_seed(SEED)

y = torch.tensor(train_label)
y_test = torch.tensor(test_label)
y_valid = torch.tensor(valid_label)

train_data = []
test_data = []
valid_data = []

for i in range(len(train_embed)):#
    temp = {}
    temp['text'] = train_embed[i]
    temp['label'] = y[i]
    train_data.append(temp)

for i in range(len(test_embed)):#
    temp = {}
    temp['text'] = test_embed[i]
    temp['label'] = y_test[i]
    test_data.append(temp)


for i in range(len(valid_embed)):#
    temp = {}
    temp['text'] = valid_embed[i]
    temp['label'] = y_valid[i]
    valid_data.append(temp)

#Get train, validation and test iterators
from torch.utils.data import DataLoader
#from torchtext import data

BATCH_SIZE = 1

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

train_iterator = DataLoader(train_data, batch_size=BATCH_SIZE)
valid_iterator = DataLoader(valid_data, shuffle = False, batch_size = BATCH_SIZE)
test_iterator = DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE)

class RNN(nn.Module):
    def __init__(self, embedding_dim, layer_dim, hidden_dim, output_dim):
        
        super().__init__()
        
        self.rnn = nn.LSTM(input_size = embedding_dim, num_layers = layer_dim, hidden_size = hidden_dim, batch_first = True, dropout = 0.4) 
        
        self.fc = nn.Linear(hidden_dim, output_dim)
        
        
    def forward(self, text):

        output, hidden = self.rnn(text)
        
        return output[:,-1,:]

LAYER_DIM = 2
HIDDEN_DIM = 256
OUTPUT_DIM = 5
EMBEDDING_DIM = 100

model = RNN(EMBEDDING_DIM, LAYER_DIM, HIDDEN_DIM, OUTPUT_DIM)

import torch.optim as optim

optimizer = optim.SGD(model.parameters(), lr=5e-2)

criterion = nn.CrossEntropyLoss()

def binary_accuracy(preds, y):
    """
    Returns accuracy per batch
    """

    pred_labels = torch.argmax(preds, dim=1)
    correct = (pred_labels == y).float()
    acc = correct.sum() / len(correct)
    return acc

def train(model, iterator, optimizer, criterion):
    
    epoch_loss = 0
    epoch_acc = 0
    
    model.train()
    ind =0
    for batch in iterator:
        
        if(len(list(batch['text'].size()))<3):
          continue
        print(ind)
        ind = ind+1
        optimizer.zero_grad()
        
        predictions = model(batch['text']).squeeze(1)

        loss = criterion(predictions, batch['label'])
        
        acc = binary_accuracy(predictions, batch['label'])
        
        loss.backward()
        
        optimizer.step()
        
        epoch_loss += loss.item()
        epoch_acc += acc.item()
        
    return epoch_loss / len(iterator), epoch_acc / len(iterator)

def evaluate(model, iterator, criterion):
    
    epoch_loss = 0
    epoch_acc = 0
    
    model.eval()
    
    with torch.no_grad():
    
        for batch in iterator:

            if(len(list(batch['text'].size()))<3):
              continue

            predictions = model(batch['text']).squeeze(1)
            
            loss = criterion(predictions, batch['label'])
            
            acc = binary_accuracy(predictions, batch['label'])

            epoch_loss += loss.item()
            epoch_acc += acc.item()
        
    return epoch_loss / len(iterator), epoch_acc / len(iterator)

N_EPOCHS = 1

best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):
    
    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)
    
    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)
    
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        for name, param in model.named_parameters():
            if param.requires_grad:
                print(name, param.data)
        torch.save(model.state_dict(), 'RNN-model.pt')
    
    print(f'Epoch: {epoch+1:02}')
    print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')

model.load_state_dict(torch.load('RNN-model.pt'))

test_loss, test_acc = evaluate(model, test_iterator, criterion)

print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')
