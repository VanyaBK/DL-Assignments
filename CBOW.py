# -*- coding: utf-8 -*-
"""CBOWipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BfhyTmMtv_YbKwXG4vXMuE4jTVXSwgd0
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from collections import Counter
from nltk.tokenize import word_tokenize
from tqdm import tqdm

class generateTrainingData(Dataset):
    def __init__(self, words_tokenized, window_size=2):

        self.context_target =  [([words_tokenized[i-(j+1)] for j in range(window_size)] +\
                                 [words_tokenized[i+(j+1)] for j in range(window_size)],
                                words_tokenized[i])
                                for i in range(window_size, len(words_tokenized)-window_size)]

        self.vocab = Counter(words_tokenized)
        file = open("CBOW_WORDS","a")
        self.word_to_idx = {word_tuple[0]: idx for idx, word_tuple in enumerate(self.vocab.most_common())}
        for item in self.word_to_idx:
          file.write("%s\n"%item)
        self.idx_to_word = list(self.word_to_idx.keys())

        self.vocab_size = len(self.vocab)
        self.window_size = window_size

    def __getitem__(self, idx):
        context = torch.tensor([self.word_to_idx[w] for w in self.context_target[idx][0]])
        target = torch.tensor([self.word_to_idx[self.context_target[idx][1]]])
        return context, target

    def __len__(self):
        return len(self.context_target)

class CBOW(nn.Module):

    def __init__(self, vocab_size, embedding_dim, window_size):
        super(CBOW, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear = nn.Linear(embedding_dim, vocab_size)
        self.window_size = window_size

    def forward(self, inputs):

        embeds = torch.sum(self.embeddings(inputs), dim=1) 
        out = self.linear(embeds) 
        log_probs = F.log_softmax(out, dim=1)

        return log_probs

WINDOWS_SIZE = 2
EMBEDDING_DIM = 64
BATCH_SIZE = 1
NUM_EPOCH = 2

from __future__ import print_function
import gensim.downloader as api # package to download text corpus
import nltk # text processing
from nltk.corpus import stopwords
import string
import numpy as np
import collections
# download stopwords
nltk.download('stopwords')

# download textcorpus
data = api.load('text8')
print(data)

# collect all words to be removed
stop = stopwords.words('english') + list(string.punctuation)

actual_words = []
cleaned_words = []
unique_words = set()

word_counts = collections.defaultdict(int)
for words in data:
    for word in words:
        word_counts[word]+=1

# remove stop words
print('removing stop words from text corpus')
for words in data:
    current_nonstop_words = [w for w in words if w not in stop]
    #cleaned_words.append(current_nonstop_words)
    cleaned_words += current_nonstop_words
    actual_words += words

    for ns in current_nonstop_words:
        unique_words.add(ns)

# print statistics
print(len(actual_words), 'words BEFORE cleaning stop words and punctuations')
print(len(cleaned_words), 'words AFTER cleaning stop words and punctuations')
print('vocabulary size: ', len(unique_words))

data = generateTrainingData(cleaned_words)
model = CBOW(len(data.vocab), EMBEDDING_DIM, WINDOWS_SIZE)
optimizer = optim.Adam(model.parameters(), lr=0.01)
loss_function = nn.NLLLoss()
losses = []
cuda_available = torch.cuda.is_available()

data_loader = DataLoader(data, batch_size=BATCH_SIZE)
#TRAIN
for epoch in range(NUM_EPOCH):
    total_loss = 0
    ind = 0
    for context, target in tqdm(data_loader):
        if context.size()[0] != BATCH_SIZE:
            continue

        if cuda_available:
            context = context.cuda()
            target = target.squeeze(1).cuda()
            model = model.cuda()

        model.zero_grad()
        log_probs = model(context)
        target.squeeze_(0)
        loss = loss_function(log_probs, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        if(ind==100000):
          break
        ind=ind+1

    losses.append(total_loss)

    print('total_loss:',total_loss)
#SAVE THE EMBEDDINGS
torch.save(embed_matrix,"CBOW_Embeddings.pt")