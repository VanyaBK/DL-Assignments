# -*- coding: utf-8 -*-
"""Skip Gram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dU1QodBYn4-mnlRZjoiS7l8HJtOjUTCd
"""

import torch
from torch.autograd import Variable
import numpy as np
import torch.functional as F
import torch.nn.functional as F

from __future__ import print_function
import gensim.downloader as api # package to download text corpus
import nltk # text processing
from nltk.corpus import stopwords
import string
import numpy as np
import collections
# download stopwords
nltk.download('stopwords')

# download textcorpus
data = api.load('text8')
print(data)

# collect all words to be removed
stop = stopwords.words('english') + list(string.punctuation)

actual_words = []
cleaned_words = []
unique_words = set()

word_counts = collections.defaultdict(int)
for words in data:
    for word in words:
        word_counts[word]+=1

# remove stop words
print('removing stop words from text corpus')
for words in data:
    current_nonstop_words = [w for w in words if w not in stop]
    cleaned_words.append(current_nonstop_words)
    #cleaned_words += current_nonstop_words
    actual_words += words

    for ns in current_nonstop_words:
        unique_words.add(ns)

# print statistics
print(len(actual_words), 'words BEFORE cleaning stop words and punctuations')
print(len(cleaned_words), 'words AFTER cleaning stop words and punctuations')
print('vocabulary size: ', len(unique_words))

# 'cleaned_words' and 'unique_words' to create a word2vec model

import collections
word_counts = collections.defaultdict(int)
for row in cleaned_words:
    for word in row:
        word_counts[word] += 1
#Generate Lookup dictionary
vocabulary_size = len(word_counts.keys())
words_list = sorted(list(word_counts.keys()),reverse=False)
word2idx = dict((word, i) for i, word in enumerate(words_list))
idx2word = dict((i, word) for i, word in enumerate(words_list))


#Generate training data
window_size = 2
idx_pairs = []

for sentence in cleaned_words:
    indices = [word2idx[word] for word in sentence]
    for center_word_pos in range(len(indices)):
        for w in range(-window_size, window_size + 1):
            context_word_pos = center_word_pos + w
            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:
                continue
            context_word_idx = indices[context_word_pos]
            idx_pairs.append((indices[center_word_pos], context_word_idx))

idx_pairs = np.array(idx_pairs)

def get_input_layer(word_idx):
    x = torch.zeros(vocabulary_size).float()
    x[word_idx] = 1.0
    return x

embedding_dims = 64
W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)
W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)
num_epochs = 1
learning_rate = 0.001

for epo in range(num_epochs):
    loss_val = 0
    index=1
    for data, target in idx_pairs:
        print(index)
        x = Variable(get_input_layer(data)).float()
        y_true = Variable(torch.from_numpy(np.array([target])).long())

        z1 = torch.matmul(W1, x)
        z2 = torch.matmul(W2, z1)
    
        log_softmax = F.log_softmax(z2, dim=0)

        loss = F.nll_loss(log_softmax.view(1,-1), y_true)
        loss_val += loss.item()
        loss.backward()
        W1.data -= learning_rate * W1.grad.data
        W2.data -= learning_rate * W2.grad.data

        W1.grad.data.zero_()
        W2.grad.data.zero_()
        index=index+1   
    print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')
#Save word embeddings
torch.save(W2,'SkipGram_Embedding.pth')